# Week 1: í™˜ê²½ êµ¬ì¶• & ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìƒì„¸ ê°€ì´ë“œ

---

## ğŸ“… Day 1-2: í”„ë¡œì íŠ¸ ì…‹ì—…

### Step 1: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ìƒì„±
mkdir ecommerce-recommendation
cd ecommerce-recommendation

# ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
mkdir -p airflow/{dags,plugins,logs,config}
mkdir -p data/{raw,processed,features}
mkdir -p models/{saved_models,experiments}
mkdir -p notebooks
mkdir -p api
mkdir -p tests
mkdir -p scripts
mkdir -p config
mkdir -p monitoring

# í™•ì¸
tree -L 2
```

**ë””ë ‰í† ë¦¬ ì„¤ëª…**:
- `airflow/dags`: Airflow DAG íŒŒì¼ë“¤
- `airflow/plugins`: ì»¤ìŠ¤í…€ Airflow í”ŒëŸ¬ê·¸ì¸
- `data/raw`: ì›ë³¸ ë°ì´í„°
- `data/processed`: ì „ì²˜ë¦¬ëœ ë°ì´í„°
- `models`: í•™ìŠµëœ ëª¨ë¸ ì €ì¥
- `notebooks`: íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
- `api`: FastAPI ì„œë¹„ìŠ¤ ì½”ë“œ
- `scripts`: ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸

---

### Step 2: Docker Composeë¡œ Airflow í™˜ê²½ êµ¬ì¶•

```bash
# docker-compose.yml ìƒì„±
vi docker-compose.yml
```

```yaml
version: '3.8'

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.8.0-python3.10
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./data:/opt/airflow/data
    - ./models:/opt/airflow/models
    - ./scripts:/opt/airflow/scripts
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    ports:
      - "5432:5432"

  redis:
    image: redis:7.2-alpine
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    user: "0:0"
    volumes:
      - ./airflow:/sources

volumes:
  postgres-db-volume:
```

---

### Step 3: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
vi .env
```

```bash
# Airflow ì„¤ì •
AIRFLOW_UID=50000
AIRFLOW_GID=0
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow1234

# PostgreSQL ì„¤ì •
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

# ì¶”ê°€ íŒ¨í‚¤ì§€
_PIP_ADDITIONAL_REQUIREMENTS=scikit-surprise pandas numpy scipy scikit-learn
```

---

### Step 4: Airflow ì‹¤í–‰

```bash
# Docker Compose ì‹¤í–‰
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f airflow-webserver

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps
```

**ì ‘ì† í™•ì¸**:
- Airflow UI: http://localhost:8080
- ID: airflow
- PW: airflow1234

---

### Step 5: Python ê°€ìƒí™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# requirements.txt ìƒì„±
vi requirements.txt
```

```txt
# Data Processing
pandas==2.1.4
numpy==1.26.2
scipy==1.11.4

# ML & Recommendation
scikit-learn==1.3.2
scikit-surprise==1.1.3

# Airflow
apache-airflow==2.8.0
apache-airflow-providers-postgres==5.10.0

# Database
psycopg2-binary==2.9.9
SQLAlchemy==2.0.23

# API
fastapi==0.109.0
uvicorn==0.25.0
redis==5.0.1

# Monitoring & MLOps
mlflow==2.9.2

# Utilities
python-dotenv==1.0.0
pyyaml==6.0.1
```

```bash
# ì„¤ì¹˜
pip install -r requirements.txt
```

---

### Step 6: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

```bash
# Kaggle API ì„¤ì¹˜ (ì´ë¯¸ ì„¤ì¹˜í–ˆë‹¤ë©´ ìŠ¤í‚µ)
pip install kaggle

# Kaggle ì¸ì¦ ì„¤ì •
mkdir -p ~/.kaggle
vi ~/.kaggle/kaggle.json
```

```json
{
  "username": "your_kaggle_username",
  "key": "your_kaggle_api_key"
}
```

```bash
# ê¶Œí•œ ì„¤ì •
chmod 600 ~/.kaggle/kaggle.json

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
cd data/raw

# eCommerce behavior data from multi category store
kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store

# ì••ì¶• í•´ì œ
unzip ecommerce-behavior-data-from-multi-category-store.zip

# í™•ì¸
ls -lh
```

**ë°ì´í„°ì…‹ êµ¬ì¡°**:
```
2019-Oct.csv  # ì•½ 42GB
2019-Nov.csv  # ì•½ 67GB
```

---

### Step 7: Git ì´ˆê¸°í™” ë° ê¸°ë³¸ ì„¤ì •

```bash
# Git ì´ˆê¸°í™”
cd ~/ecommerce-recommendation
git init

# .gitignore ìƒì„±
vi .gitignore
```

```txt
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/

# Airflow
airflow/logs/
airflow.db
airflow.cfg

# Data
data/raw/*.csv
data/processed/*.csv
*.parquet

# Models
models/saved_models/*
!models/saved_models/.gitkeep

# Jupyter
.ipynb_checkpoints/
*.ipynb

# Environment
.env
.env.local

# OS
.DS_Store
Thumbs.db

# IDE
.vscode/
.idea/
```

```bash
# ì²« ì»¤ë°‹
git add .
git commit -m "Initial project setup with Airflow and Docker"
```

---

## ğŸ“… Day 3-4: ë°ì´í„° ìˆ˜ì§‘ DAG êµ¬ì¶•

### Step 1: ë°ì´í„° íƒìƒ‰ (EDA)

```bash
# Jupyter Notebook ì‹¤í–‰
jupyter notebook
```

**notebooks/01_eda.ipynb** ìƒì„±:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§)
df = pd.read_csv('../data/raw/2019-Oct.csv', nrows=1000000)

# ê¸°ë³¸ ì •ë³´
print(df.head())
print(df.info())
print(df.describe())

# ì»¬ëŸ¼ í™•ì¸
print(df.columns)
# ['event_time', 'event_type', 'product_id', 'category_id', 
#  'category_code', 'brand', 'price', 'user_id', 'user_session']

# ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum())

# event_type ë¶„í¬
print(df['event_type'].value_counts())
# view        ~89%
# cart        ~8%
# purchase    ~3%

# ì‹œê°„ëŒ€ë³„ ë¶„í¬
df['event_time'] = pd.to_datetime(df['event_time'])
df['hour'] = df['event_time'].dt.hour
df['hour'].hist(bins=24)
plt.title('Events by Hour')
plt.xlabel('Hour')
plt.ylabel('Count')
plt.show()

# ìœ ì €ë‹¹ ì´ë²¤íŠ¸ ìˆ˜
user_events = df.groupby('user_id').size()
print(f"í‰ê·  ì´ë²¤íŠ¸/ìœ ì €: {user_events.mean():.2f}")
print(f"ì¤‘ì•™ê°’: {user_events.median():.2f}")

# ìƒí’ˆ ì¸ê¸°ë„
top_products = df['product_id'].value_counts().head(20)
print("Top 20 ì œí’ˆ:")
print(top_products)
```

---

### Step 2: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„

```bash
# SQL ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
vi scripts/create_tables.sql
```

```sql
-- ìœ ì € í–‰ë™ ì´ë²¤íŠ¸ í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS user_events (
    event_id SERIAL PRIMARY KEY,
    event_time TIMESTAMP NOT NULL,
    event_type VARCHAR(20) NOT NULL,
    product_id BIGINT NOT NULL,
    category_id BIGINT,
    category_code VARCHAR(200),
    brand VARCHAR(100),
    price DECIMAL(10, 2),
    user_id BIGINT NOT NULL,
    user_session UUID NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_user_events_user_id ON user_events(user_id);
CREATE INDEX idx_user_events_product_id ON user_events(product_id);
CREATE INDEX idx_user_events_event_time ON user_events(event_time);
CREATE INDEX idx_user_events_event_type ON user_events(event_type);

-- ìƒí’ˆ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS products (
    product_id BIGINT PRIMARY KEY,
    category_id BIGINT,
    category_code VARCHAR(200),
    brand VARCHAR(100),
    avg_price DECIMAL(10, 2),
    view_count INT DEFAULT 0,
    cart_count INT DEFAULT 0,
    purchase_count INT DEFAULT 0,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ìœ ì € í†µê³„ í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS user_stats (
    user_id BIGINT PRIMARY KEY,
    total_views INT DEFAULT 0,
    total_carts INT DEFAULT 0,
    total_purchases INT DEFAULT 0,
    total_spent DECIMAL(12, 2) DEFAULT 0,
    first_event TIMESTAMP,
    last_event TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

```bash
# PostgreSQL ì ‘ì†í•˜ì—¬ í…Œì´ë¸” ìƒì„±
docker exec -it ecommerce-recommendation-postgres-1 psql -U airflow -d airflow

# SQL ì‹¤í–‰
\i /opt/airflow/scripts/create_tables.sql

# í…Œì´ë¸” í™•ì¸
\dt

# ì¢…ë£Œ
\q
```

---

### Step 3: ë°ì´í„° ìˆ˜ì§‘ ìœ í‹¸ë¦¬í‹° ì‘ì„±

```bash
# scripts/data_loader.py ìƒì„±
vi scripts/data_loader.py
```

```python
import pandas as pd
import psycopg2
from psycopg2.extras import execute_batch
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataLoader:
    def __init__(self, db_config):
        self.db_config = db_config
        self.conn = None
        
    def connect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = psycopg2.connect(**self.db_config)
            logger.info("Database connected successfully")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed")
    
    def load_csv_to_db(self, csv_path, table_name, batch_size=10000):
        """CSV íŒŒì¼ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ"""
        try:
            # CSV ì½ê¸° (ì²­í¬ ë‹¨ìœ„)
            chunk_iter = pd.read_csv(csv_path, chunksize=batch_size)
            
            cursor = self.conn.cursor()
            total_rows = 0
            
            for i, chunk in enumerate(chunk_iter):
                # ë°ì´í„° ì „ì²˜ë¦¬
                chunk = self._preprocess_chunk(chunk)
                
                # ë°°ì¹˜ ì¸ì„œíŠ¸
                insert_query = self._get_insert_query(table_name)
                records = chunk.to_records(index=False).tolist()
                
                execute_batch(cursor, insert_query, records, page_size=batch_size)
                self.conn.commit()
                
                total_rows += len(chunk)
                logger.info(f"Batch {i+1}: {total_rows} rows loaded")
                
                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì£¼ê¸°ì  ì»¤ë°‹
                if (i + 1) % 10 == 0:
                    self.conn.commit()
            
            cursor.close()
            logger.info(f"Total {total_rows} rows loaded to {table_name}")
            
        except Exception as e:
            self.conn.rollback()
            logger.error(f"Error loading data: {e}")
            raise
    
    def _preprocess_chunk(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # ì»¬ëŸ¼ëª… ë³€ê²½
        df = df.rename(columns={
            'event_time': 'event_time',
            'event_type': 'event_type',
            'product_id': 'product_id',
            'category_id': 'category_id',
            'category_code': 'category_code',
            'brand': 'brand',
            'price': 'price',
            'user_id': 'user_id',
            'user_session': 'user_session'
        })
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df['category_id'] = df['category_id'].fillna(0)
        df['category_code'] = df['category_code'].fillna('unknown')
        df['brand'] = df['brand'].fillna('unknown')
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        df['event_time'] = pd.to_datetime(df['event_time'])
        
        return df
    
    def _get_insert_query(self, table_name):
        """ì¸ì„œíŠ¸ ì¿¼ë¦¬ ìƒì„±"""
        if table_name == 'user_events':
            return """
                INSERT INTO user_events 
                (event_time, event_type, product_id, category_id, 
                 category_code, brand, price, user_id, user_session)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT DO NOTHING
            """
        else:
            raise ValueError(f"Unknown table: {table_name}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    db_config = {
        'host': 'localhost',
        'port': 5432,
        'database': 'airflow',
        'user': 'airflow',
        'password': 'airflow'
    }
    
    loader = DataLoader(db_config)
    loader.connect()
    
    # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ (ì „ì²´ëŠ” ë„ˆë¬´ í¬ë¯€ë¡œ 10ì›” ì²« ë‚ ë§Œ)
    loader.load_csv_to_db(
        csv_path='../data/raw/2019-Oct.csv',
        table_name='user_events',
        batch_size=10000
    )
    
    loader.close()
```

---

### Step 4: Airflow DAG ì‘ì„± - ë°ì´í„° ìˆ˜ì§‘

```bash
# airflow/dags/01_data_collection_dag.py ìƒì„±
vi airflow/dags/01_data_collection_dag.py
```

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import pandas as pd
import logging

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'data_collection_pipeline',
    default_args=default_args,
    description='ìˆ˜ì§‘ ë° ì ì¬ íŒŒì´í”„ë¼ì¸',
    schedule_interval='@daily',  # ë§¤ì¼ ì‹¤í–‰
    catchup=False,
    tags=['data-collection', 'ecommerce'],
)

def extract_daily_events(**context):
    """ì¼ì¼ ì´ë²¤íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
    execution_date = context['ds']  # YYYY-MM-DD
    logging.info(f"Extracting events for {execution_date}")
    
    # ì‹¤ì œë¡œëŠ” APIë‚˜ ë¡œê·¸ ì‹œìŠ¤í…œì—ì„œ ê°€ì ¸ì˜´
    # ì—¬ê¸°ì„œëŠ” CSV ìƒ˜í”Œë§
    df = pd.read_csv(
        '/opt/airflow/data/raw/2019-Oct.csv',
        nrows=100000  # ìƒ˜í”Œ
    )
    
    # ë‚ ì§œ í•„í„°ë§ (ì‹¤ì œ í™˜ê²½ì—ì„œ)
    df['event_time'] = pd.to_datetime(df['event_time'])
    df = df[df['event_time'].dt.date == pd.to_datetime(execution_date).date()]
    
    # ì„ì‹œ ì €ì¥
    output_path = f'/opt/airflow/data/raw/events_{execution_date}.csv'
    df.to_csv(output_path, index=False)
    
    logging.info(f"Extracted {len(df)} events to {output_path}")
    return output_path

def validate_data(**context):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    task_instance = context['task_instance']
    file_path = task_instance.xcom_pull(task_ids='extract_events')
    
    df = pd.read_csv(file_path)
    
    # ê²€ì¦ ê·œì¹™
    checks = {
        'row_count': len(df) > 0,
        'required_columns': all(col in df.columns for col in 
            ['event_time', 'event_type', 'product_id', 'user_id']),
        'no_null_user_id': df['user_id'].notnull().all(),
        'valid_price': (df['price'] >= 0).all(),
    }
    
    if not all(checks.values()):
        failed_checks = [k for k, v in checks.items() if not v]
        raise ValueError(f"Data validation failed: {failed_checks}")
    
    logging.info("Data validation passed")
    return True

def load_to_postgres(**context):
    """PostgreSQLì— ì ì¬"""
    task_instance = context['task_instance']
    file_path = task_instance.xcom_pull(task_ids='extract_events')
    
    df = pd.read_csv(file_path)
    
    # PostgresHook ì‚¬ìš©
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    engine = pg_hook.get_sqlalchemy_engine()
    
    # ë°°ì¹˜ ì¸ì„œíŠ¸
    df.to_sql(
        'user_events',
        engine,
        if_exists='append',
        index=False,
        method='multi',
        chunksize=1000
    )
    
    logging.info(f"Loaded {len(df)} rows to user_events table")

def update_product_stats(**context):
    """ìƒí’ˆ í†µê³„ ì—…ë°ì´íŠ¸"""
    execution_date = context['ds']
    
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    conn = pg_hook.get_conn()
    cursor = conn.cursor()
    
    # ìƒí’ˆë³„ ì§‘ê³„
    query = f"""
        INSERT INTO products (product_id, category_id, category_code, brand, 
                             view_count, cart_count, purchase_count, avg_price)
        SELECT 
            product_id,
            MAX(category_id) as category_id,
            MAX(category_code) as category_code,
            MAX(brand) as brand,
            SUM(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END) as view_count,
            SUM(CASE WHEN event_type = 'cart' THEN 1 ELSE 0 END) as cart_count,
            SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchase_count,
            AVG(price) as avg_price
        FROM user_events
        WHERE DATE(event_time) = '{execution_date}'
        GROUP BY product_id
        ON CONFLICT (product_id) 
        DO UPDATE SET
            view_count = products.view_count + EXCLUDED.view_count,
            cart_count = products.cart_count + EXCLUDED.cart_count,
            purchase_count = products.purchase_count + EXCLUDED.purchase_count,
            avg_price = (products.avg_price + EXCLUDED.avg_price) / 2,
            updated_at = CURRENT_TIMESTAMP;
    """
    
    cursor.execute(query)
    conn.commit()
    cursor.close()
    conn.close()
    
    logging.info("Product stats updated")

# Task ì •ì˜
extract_task = PythonOperator(
    task_id='extract_events',
    python_callable=extract_daily_events,
    dag=dag,
)

validate_task = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load_to_postgres',
    python_callable=load_to_postgres,
    dag=dag,
)

stats_task = PythonOperator(
    task_id='update_product_stats',
    python_callable=update_product_stats,
    dag=dag,
)

# Task ì˜ì¡´ì„±
extract_task >> validate_task >> load_task >> stats_task
```

---

### Step 5: Airflow Connection ì„¤ì •

```bash
# Airflow UIì—ì„œ ì„¤ì •
# Admin > Connections > + ë²„íŠ¼ í´ë¦­

# Connection Id: postgres_default
# Connection Type: Postgres
# Host: postgres
# Schema: airflow
# Login: airflow
# Password: airflow
# Port: 5432
```

ë˜ëŠ” CLIë¡œ:

```bash
docker exec -it ecommerce-recommendation-airflow-webserver-1 bash

airflow connections add 'postgres_default' \
    --conn-type 'postgres' \
    --conn-host 'postgres' \
    --conn-schema 'airflow' \
    --conn-login 'airflow' \
    --conn-password 'airflow' \
    --conn-port 5432
```

---

### Step 6: DAG í…ŒìŠ¤íŠ¸

```bash
# DAG êµ¬ë¬¸ ê²€ì‚¬
docker exec -it ecommerce-recommendation-airflow-webserver-1 bash
airflow dags list | grep data_collection

# DAG í…ŒìŠ¤íŠ¸ ì‹¤í–‰
airflow dags test data_collection_pipeline 2025-01-01

# íŠ¹ì • Taskë§Œ í…ŒìŠ¤íŠ¸
airflow tasks test data_collection_pipeline extract_events 2025-01-01
```

---

## ğŸ“… Day 5-7: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### Step 1: ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‘ì„±

```bash
vi scripts/preprocessing.py
```

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataPreprocessor:
    def __init__(self):
        self.label_encoders = {}
    
    def remove_duplicates(self, df):
        """ì¤‘ë³µ ì œê±°"""
        before = len(df)
        df = df.drop_duplicates(
            subset=['user_id', 'product_id', 'event_time', 'event_type']
        )
        after = len(df)
        logger.info(f"Removed {before - after} duplicates")
        return df
    
    def handle_missing_values(self, df):
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
        # ì¹´í…Œê³ ë¦¬ ê²°ì¸¡ì¹˜
        df['category_code'] = df['category_code'].fillna('unknown')
        df['brand'] = df['brand'].fillna('unknown')
        df['category_id'] = df['category_id'].fillna(0)
        
        # ê°€ê²© ê²°ì¸¡ì¹˜ (ì¹´í…Œê³ ë¦¬ í‰ê· ìœ¼ë¡œ ëŒ€ì²´)
        df['price'] = df.groupby('category_code')['price'].transform(
            lambda x: x.fillna(x.median())
        )
        
        logger.info("Missing values handled")
        return df
    
    def filter_active_users(self, df, min_events=5):
        """í™œì„± ìœ ì € í•„í„°ë§"""
        user_event_counts = df.groupby('user_id').size()
        active_users = user_event_counts[user_event_counts >= min_events].index
        
        before = len(df)
        df = df[df['user_id'].isin(active_users)]
        after = len(df)
        
        logger.info(f"Filtered to {len(active_users)} active users")
        logger.info(f"Removed {before - after} rows from inactive users")
        return df
    
    def filter_popular_products(self, df, min_interactions=10):
        """ì¸ê¸° ìƒí’ˆ í•„í„°ë§"""
        product_counts = df.groupby('product_id').size()
        popular_products = product_counts[product_counts >= min_interactions].index
        
        before = len(df)
        df = df[df['product_id'].isin(popular_products)]
        after = len(df)
        
        logger.info(f"Filtered to {len(popular_products)} popular products")
        logger.info(f"Removed {before - after} rows from unpopular products")
        return df
    
    def encode_categorical_features(self, df):
        """ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¸ì½”ë”©"""
        categorical_cols = ['event_type', 'category_code', 'brand']
        
        for col in categorical_cols:
            if col not in self.label_encoders:
                self.label_encoders[col] = LabelEncoder()
                df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col])
            else:
                # ìƒˆë¡œìš´ ê°’ ì²˜ë¦¬
                df[f'{col}_encoded'] = df[col].apply(
                    lambda x: self.label_encoders[col].transform([x])[0] 
                    if x in self.label_encoders[col].classes_ else -1
                )
        
        logger.info("Categorical features encoded")
        return df
    
    def create_temporal_features(self, df):
        """ì‹œê°„ ê¸°ë°˜ í”¼ì²˜ ìƒì„±"""
        df['event_time'] = pd.to_datetime(df['event_time'])
        
        df['hour'] = df['event_time'].dt.hour
        df['day_of_week'] = df['event_time'].dt.dayofweek
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        df['month'] = df['event_time'].dt.month
        
        # ì‹œê°„ëŒ€ êµ¬ë¶„
        df['time_of_day'] = pd.cut(
            df['hour'],
            bins=[0, 6, 12, 18, 24],
            labels=['night', 'morning', 'afternoon', 'evening'],
            include_lowest=True
        )
        
        logger.info("Temporal features created")
        return df
    
    def create_user_features(self, df):
        """ìœ ì € í”¼ì²˜ ìƒì„±"""
        user_features = df.groupby('user_id').agg({
            'event_time': ['min', 'max', 'count'],
            'product_id': 'nunique',
            'price': ['mean', 'sum'],
            'event_type': lambda x: (x == 'purchase').sum()
        }).reset_index()
        
        user_features.columns = [
            'user_id', 'first_event', 'last_event', 'total_events',
            'unique_products', 'avg_price', 'total_spent', 'purchase_count'
        ]
        
        # í™œë™ ê¸°ê°„
        user_features['active_days'] = (
            user_features['last_event'] - user_features['first_event']
        ).dt.days + 1
        
        # ì¼í‰ê·  ì´ë²¤íŠ¸
        user_features['events_per_day'] = (
            user_features['total_events'] / user_features['active_days']
        )
        
        # êµ¬ë§¤ ì „í™˜ìœ¨
        user_features['purchase_rate'] = (
            user_features['purchase_count'] / user_features['total_events']
        )
        
        logger.info(f"Created features for {len(user_features)} users")
        return user_features
    
    def create_product_features(self, df):
        """ìƒí’ˆ í”¼ì²˜ ìƒì„±"""
        product_features = df.groupby('product_id').agg({
            'event_type': [
                lambda x: (x == 'view').sum(),
                lambda x: (x == 'cart').sum(),
                lambda x: (x == 'purchase').sum()
            ],
            'user_id': 'nunique',
            'price': 'mean',
            'category_code': 'first',
            'brand': 'first'
        }).reset_index()
        
        product_features.columns = [
            'product_id', 'view_count', 'cart_count', 'purchase_count',
            'unique_users', 'avg_price', 'category_code', 'brand'
        ]
        
        # ì „í™˜ìœ¨
        product_features['cart_to_view_rate'] = (
            product_features['cart_count'] / 
            (product_features['view_count'] + 1)
        )
        product_features['purchase_to_cart_rate'] = (
            product_features['purchase_count'] / 
            (product_features['cart_count'] + 1)
        )
        
        # ì¸ê¸°ë„ ì ìˆ˜
        product_features['popularity_score'] = (
            product_features['view_count'] * 0.1 +
            product_features['cart_count'] * 0.3 +
            product_features['purchase_count'] * 0.6
        )
        
        logger.info(f"Created features for {len(product_features)} products")
        return product_features
    
    def create_interaction_matrix(self, df, event_type='purchase'):
        """ìœ ì €-ì•„ì´í…œ ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        # íŠ¹ì • ì´ë²¤íŠ¸ë§Œ í•„í„°ë§
        interactions = df[df['event_type'] == event_type].copy()
        
        # ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ìµœê·¼ ì´ë²¤íŠ¸ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
        max_time = interactions['event_time'].max()
        interactions['days_ago'] = (
            max_time - interactions['event_time']
        ).dt.days
        interactions['weight'] = np.exp(-interactions['days_ago'] / 30)
        
        # ìœ ì €-ì•„ì´í…œ ìŒì˜ ì´ ê°€ì¤‘ì¹˜
        matrix = interactions.groupby(
            ['user_id', 'product_id']
        )['weight'].sum().reset_index()
        
        matrix.columns = ['user_id', 'product_id', 'rating']
        
        logger.info(f"Created interaction matrix: {len(matrix)} interactions")
        return matrix
    
    def preprocess_pipeline(self, df):
        """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        logger.info("Starting preprocessing pipeline")
        
        df = self.remove_duplicates(df)
        df = self.handle_missing_values(df)
        df = self.filter_active_users(df, min_events=5)
        df = self.filter_popular_products(df, min_interactions=10)
        df = self.encode_categorical_features(df)
        df = self.create_temporal_features(df)
        
        logger.info("Preprocessing pipeline completed")
        return df

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    preprocessor = DataPreprocessor()
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('../data/raw/2019-Oct.csv', nrows=100000)
    
    # ì „ì²˜ë¦¬
    df_processed = preprocessor.preprocess_pipeline(df)
    
    # í”¼ì²˜ ìƒì„±
    user_features = preprocessor.create_user_features(df_processed)
    product_features = preprocessor.create_product_features(df_processed)
    interaction_matrix = preprocessor.create_interaction_matrix(df_processed)
    
    # ì €ì¥
    df_processed.to_csv('../data/processed/events_processed.csv', index=False)
    user_features.to_csv('../data/features/user_features.csv', index=False)
    product_features.to_csv('../data/features/product_features.csv', index=False)
    interaction_matrix.to_csv('../data/features/interaction_matrix.csv', index=False)
    
    print("Preprocessing completed!")
```

---

### Step 2: ì „ì²˜ë¦¬ DAG ì‘ì„±

```bash
vi airflow/dags/02_preprocessing_dag.py
```

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import pandas as pd
import sys
sys.path.append('/opt/airflow/scripts')
from preprocessing import DataPreprocessor
import logging

default_args = {
    'owner': 'data-team',
    'depends_on_past': True,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'data_preprocessing_pipeline',
    default_args=default_args,
    description='ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§',
    schedule_interval='@daily',
    catchup=False,
    tags=['preprocessing', 'feature-engineering'],
)

def extract_from_db(**context):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì›ë³¸ ë°ì´í„° ì¶”ì¶œ"""
    execution_date = context['ds']
    
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # ìµœê·¼ 30ì¼ ë°ì´í„° ì¶”ì¶œ
    query = f"""
        SELECT *
        FROM user_events
        WHERE event_time >= '{execution_date}'::date - INTERVAL '30 days'
          AND event_time < '{execution_date}'::date + INTERVAL '1 day'
    """
    
    df = pg_hook.get_pandas_df(query)
    logging.info(f"Extracted {len(df)} rows from database")
    
    # ì„ì‹œ ì €ì¥
    output_path = f'/opt/airflow/data/raw/raw_{execution_date}.csv'
    df.to_csv(output_path, index=False)
    
    return output_path

def preprocess_data(**context):
    """ë°ì´í„° ì „ì²˜ë¦¬"""
    task_instance = context['task_instance']
    input_path = task_instance.xcom_pull(task_ids='extract_from_db')
    execution_date = context['ds']
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(input_path)
    logging.info(f"Loaded {len(df)} rows for preprocessing")
    
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    preprocessor = DataPreprocessor()
    df_processed = preprocessor.preprocess_pipeline(df)
    
    # ì €ì¥
    output_path = f'/opt/airflow/data/processed/processed_{execution_date}.csv'
    df_processed.to_csv(output_path, index=False)
    
    logging.info(f"Preprocessing completed: {len(df_processed)} rows")
    return output_path

def create_user_features(**context):
    """ìœ ì € í”¼ì²˜ ìƒì„±"""
    task_instance = context['task_instance']
    input_path = task_instance.xcom_pull(task_ids='preprocess_data')
    execution_date = context['ds']
    
    df = pd.read_csv(input_path)
    
    preprocessor = DataPreprocessor()
    user_features = preprocessor.create_user_features(df)
    
    # PostgreSQLì— ì €ì¥
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    engine = pg_hook.get_sqlalchemy_engine()
    
    user_features.to_sql(
        'user_features',
        engine,
        if_exists='replace',
        index=False
    )
    
    logging.info(f"Created features for {len(user_features)} users")

def create_product_features(**context):
    """ìƒí’ˆ í”¼ì²˜ ìƒì„±"""
    task_instance = context['task_instance']
    input_path = task_instance.xcom_pull(task_ids='preprocess_data')
    execution_date = context['ds']
    
    df = pd.read_csv(input_path)
    
    preprocessor = DataPreprocessor()
    product_features = preprocessor.create_product_features(df)
    
    # PostgreSQLì— ì €ì¥
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    engine = pg_hook.get_sqlalchemy_engine()
    
    product_features.to_sql(
        'product_features',
        engine,
        if_exists='replace',
        index=False
    )
    
    logging.info(f"Created features for {len(product_features)} products")

def create_interaction_matrix(**context):
    """ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
    task_instance = context['task_instance']
    input_path = task_instance.xcom_pull(task_ids='preprocess_data')
    execution_date = context['ds']
    
    df = pd.read_csv(input_path)
    
    preprocessor = DataPreprocessor()
    
    # êµ¬ë§¤ ê¸°ë°˜ ë§¤íŠ¸ë¦­ìŠ¤
    purchase_matrix = preprocessor.create_interaction_matrix(df, 'purchase')
    purchase_matrix.to_csv(
        f'/opt/airflow/data/features/purchase_matrix_{execution_date}.csv',
        index=False
    )
    
    # ì¥ë°”êµ¬ë‹ˆ ê¸°ë°˜ ë§¤íŠ¸ë¦­ìŠ¤
    cart_matrix = preprocessor.create_interaction_matrix(df, 'cart')
    cart_matrix.to_csv(
        f'/opt/airflow/data/features/cart_matrix_{execution_date}.csv',
        index=False
    )
    
    logging.info("Interaction matrices created")

# Task ì •ì˜
extract_task = PythonOperator(
    task_id='extract_from_db',
    python_callable=extract_from_db,
    dag=dag,
)

preprocess_task = PythonOperator(
    task_id='preprocess_data',
    python_callable=preprocess_data,
    dag=dag,
)

user_features_task = PythonOperator(
    task_id='create_user_features',
    python_callable=create_user_features,
    dag=dag,
)

product_features_task = PythonOperator(
    task_id='create_product_features',
    python_callable=create_product_features,
    dag=dag,
)

interaction_matrix_task = PythonOperator(
    task_id='create_interaction_matrix',
    python_callable=create_interaction_matrix,
    dag=dag,
)

# Task ì˜ì¡´ì„±
extract_task >> preprocess_task >> [
    user_features_task,
    product_features_task,
    interaction_matrix_task
]
```

---

## âœ… Week 1 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. Docker ì„œë¹„ìŠ¤ í™•ì¸
docker-compose ps

# 2. Airflow UI ì ‘ì† í™•ì¸
# http://localhost:8080

# 3. PostgreSQL í…Œì´ë¸” í™•ì¸
docker exec -it ecommerce-recommendation-postgres-1 psql -U airflow -d airflow -c "\dt"

# 4. DAG ëª©ë¡ í™•ì¸
docker exec -it ecommerce-recommendation-airflow-webserver-1 airflow dags list

# 5. ìƒ˜í”Œ ë°ì´í„° í™•ì¸
docker exec -it ecommerce-recommendation-postgres-1 psql -U airflow -d airflow -c "SELECT COUNT(*) FROM user_events;"

# 6. ì „ì²˜ë¦¬ëœ íŒŒì¼ í™•ì¸
ls -lh data/processed/
ls -lh data/features/
```

---

## ğŸ¯ Week 1 ìµœì¢… ì ê²€

### ë‹¬ì„± ëª©í‘œ:
- âœ… Airflow + PostgreSQL í™˜ê²½ êµ¬ì¶• ì™„ë£Œ
- âœ… ë°ì´í„° ìˆ˜ì§‘ DAG ì‘ì„± ë° í…ŒìŠ¤íŠ¸
- âœ… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ
- âœ… ìœ ì €-ì•„ì´í…œ ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±

### ë‹¤ìŒ ì£¼ ì¤€ë¹„ì‚¬í•­:
- Week 2ì—ì„œëŠ” ì´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ
- í˜‘ì—… í•„í„°ë§ & ì»¨í…ì¸  ê¸°ë°˜ í•„í„°ë§ êµ¬í˜„
- ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ìµœì í™”

---

**Week 1 ì™„ë£Œ! ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ğŸ‰**

ê¶ê¸ˆí•œ ì ì´ë‚˜ ë§‰íˆëŠ” ë¶€ë¶„ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!